{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "407_Musat_Bianca.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCkNrzND6gVk",
        "outputId": "635252b0-aa1d-4530-98e7-03f711b4d768"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python==3.4.2.17\n",
        "!pip install opencv-contrib-python==3.4.2.17\n",
        "\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import sklearn\n",
        "import keras\n",
        "import tensorflow\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b5vBVNM5VkA",
        "outputId": "0613b9c5-3fdf-46a8-88db-4a6383289d9a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==3.4.2.17) (1.21.6)\n",
            "Requirement already satisfied: opencv-contrib-python==3.4.2.17 in /usr/local/lib/python3.7/dist-packages (3.4.2.17)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "print('Numpy version: ', np.__version__)\n",
        "print('OpenCV2 version: ', cv.__version__)\n",
        "print('Sklearn version: ', sklearn.__version__)\n",
        "print('Keras version: ', keras.__version__)\n",
        "print('Tensorflow version: ', tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii_62JxYQDPf",
        "outputId": "04b6ed67-f1c2-4c3a-dc26-d6436ddb0317"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n",
            "Numpy version:  1.21.6\n",
            "OpenCV2 version:  3.4.2\n",
            "Sklearn version:  1.0.2\n",
            "Keras version:  2.8.0\n",
            "Tensorflow version:  2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_FOLDER = 'test'  # this defines the folder with the input data; change to test for testing file "
      ],
      "metadata": {
        "id": "DwUN5avLN7Fc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Board definition.\n",
        "\n",
        "For the board, we will define a template that encodes the initial information in each patch (1 -> triple word, 2 -> double word, 3 -> triple letter, 4 -> double letter, 0 -> otherwise). Each board will also contain the letters or replacement letter (the empty box) once they are extracted from each image. The replacement letter will be denoted by '-' on the board representation."
      ],
      "metadata": {
        "id": "mfjAjUy6uoDX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Wrdb1tAH4BaZ"
      },
      "outputs": [],
      "source": [
        "# the dictionary of the values of each letter\n",
        "letter_score = {'A':1, 'B':3, 'C':3, 'D':2, 'E':1, 'F':4, 'G':2, 'H':4, 'I':1, 'J': 8, 'K':5, 'L':1, 'M':3, 'N':1, 'O':1, 'P':3, 'Q':10, 'R':1, 'S':1, 'T':1, 'U':1, 'V':4, 'W':4, 'X':8, 'Y':4, 'Z':10, '-':0}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NO_SQUARES = 15  # number of squares on the board (on each row/col)\n",
        "\n",
        "def no_to_letter(indx):\n",
        "    \"\"\"\n",
        "    The mapping from indices to letters for the column letter representation (0->a, 1->b, 2->c etc)\n",
        "    \"\"\"\n",
        "    return chr(ord('`') + indx + 1)"
      ],
      "metadata": {
        "id": "MnveHpmG70xB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the positions of the premium squares\n",
        "prem_sq_positions = {'triple-word' : [(0, 0), (0, 7), (0, 14), (7, 0), (7, 14), (14, 0), (14, 7), (14, 14)], 'double-word' : [(1, 1), (2, 2), (3, 3), (4, 4), (1, 13), (2, 12), (3, 11), (4, 10), (7, 7), (10, 4), (11, 3), (12, 2), (13, 1), (10, 10), (11, 11), (12, 12), (13, 13)], 'triple-letter' : [(1, 5), (1, 9), (5, 5), (5, 9), (9, 5), (9, 9), (13, 5), (13, 9), (5, 1), (9, 1), (5, 13), (9, 13)], 'double-letter': [(0, 3), (0, 11), (2, 6), (2, 8), (3, 0), (3, 7), (3, 14), (6, 2), (6, 6), (6, 8), (6, 12), (7, 3), (7, 11), (8, 2), (8, 6), (8, 8), (8, 12), (11, 0), (11, 7), (11, 14), (12, 6), (12, 8), (14, 3), (14, 11)]}"
      ],
      "metadata": {
        "id": "h4_hEM_95VCR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the premium square name mapped to a number for the template board (1 -> triple word, 2 -> double word, 3 -> triple letter, 4 -> double letter)\n",
        "sq_to_number = {'triple-word' : 1, 'double-word' : 2, 'triple-letter' : 3, 'double-letter': 4}"
      ],
      "metadata": {
        "id": "QClPogzKKoP1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_board_template():\n",
        "    \"\"\"\n",
        "    The template for the board (1 -> triple word, 2 -> double word, 3 -> triple letter, 4 -> double letter, 0 otherwise)\n",
        "    \"\"\"\n",
        "    board_template = [[0 for _ in range(NO_SQUARES)] for _ in range(NO_SQUARES)]\n",
        "    for i in range(NO_SQUARES):\n",
        "        for j in range(NO_SQUARES):\n",
        "            for prem_sq in prem_sq_positions:\n",
        "                if (i, j) in prem_sq_positions[prem_sq]:\n",
        "                    board_template[i][j] = sq_to_number[prem_sq]\n",
        "    return board_template"
      ],
      "metadata": {
        "id": "8hFhlzCNKWUX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "board_template = get_board_template()"
      ],
      "metadata": {
        "id": "s5wNAWJDLvZi"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scoring computations\n",
        "\n",
        "In order to compute the score of each board, we will need the previous board (which initially is the empty template) and the current board. Then, we will extarct all the new letters from the current board and filter them so that we do not have isolated letters that are the result of an error in the letter recognition model (more about it below). Then, we will check if there are any replacement letters identified and again filter them.\n",
        "We will then extract each vertical and horizontal word from both current and previous boards and then, for the new words, we will compute the score."
      ],
      "metadata": {
        "id": "6HaD3G8YVWJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def are_neigh_around(new_letters_temp, i1, j1):\n",
        "    \"\"\"\n",
        "    It checks if the current letter have any neighbours around it or is isolated.\n",
        "    If it is isolated it is likely to be an error from the CNN which recognize the letters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    new_letters_temp : list\n",
        "        The list of new letters on the board.\n",
        "    i1 : int\n",
        "        The row of the word    \n",
        "    j1 : int\n",
        "        The col of the word\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    have_neigh: bool\n",
        "       True if it has heigbors (maximum distance between letters is 2).\n",
        "    \"\"\"\n",
        "    have_neigh = False\n",
        "    for [l, i, j] in new_letters_temp:\n",
        "        if i == i1 and j == j1:\n",
        "            continue\n",
        "        if abs(i - i1) < 2 or abs(j - j1) < 2:\n",
        "            have_neigh = True\n",
        "    return have_neigh"
      ],
      "metadata": {
        "id": "aH_sQCuSVKY-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_false_new_letters(new_letters, new_letters_temp):\n",
        "    \"\"\"\n",
        "    It checks if there are isolated letters whcich are likely to be an error from the CNN which recognize the letters.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    new_letters : list\n",
        "        The list of new letters on the board and their coordinates as they should be outputed.\n",
        "    new_letters_temp : list\n",
        "        The list of new letters on the board and their coordinates in the matrix representig the board.\n",
        "    Returns\n",
        "    -------\n",
        "    new_letters_real, new_letters_temp_real: list, list\n",
        "       The letters that are not isolated\n",
        "    \"\"\"\n",
        "    new_letters_real = []\n",
        "    new_letters_temp_real = []\n",
        "    i_set = set()\n",
        "    i_dict = {}\n",
        "    j_set = set()\n",
        "    j_dict = {}\n",
        "    for [l, i, j] in new_letters_temp:\n",
        "        if i in i_dict:\n",
        "            i_dict[i] += 1\n",
        "        else:\n",
        "            i_dict[i] = 0\n",
        "        if j in j_dict:\n",
        "            j_dict[j] += 1\n",
        "        else:\n",
        "            j_dict[j] = 0\n",
        "        i_set.add(i)\n",
        "        j_set.add(j)\n",
        "    if len(i_set) < len(j_set):  # horizontal word\n",
        "        max_key = max(i_dict, key=i_dict.get)  # the most common row\n",
        "        for indx, [l, i, j] in enumerate(new_letters_temp):\n",
        "            if i != max_key:  # if the letter is not on the same row with the newly added word, it must be an outsider\n",
        "                continue\n",
        "            if not are_neigh_around(new_letters_temp, i, j):  # if the letter has no neighbors, it must be an outsider\n",
        "                continue\n",
        "            new_letters_temp_real.append([l, i, j])\n",
        "            new_letters_real.append(new_letters[indx])\n",
        "    else: # vertical word\n",
        "        max_key = max(j_dict, key=j_dict.get)  # the most common col\n",
        "        for indx, [l, i, j] in enumerate(new_letters_temp):\n",
        "            if j != max_key:  # if the letter is not on the same col with the newly added word, it must be an outsider\n",
        "                continue\n",
        "            if not are_neigh_around(new_letters_temp, i, j):  # if the letter has no neighbors, it must be an outsider\n",
        "                continue\n",
        "            new_letters_temp_real.append([l, i, j])\n",
        "            new_letters_real.append(new_letters[indx])\n",
        "    return new_letters_real, new_letters_temp_real"
      ],
      "metadata": {
        "id": "RWcx8Du5RNVh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def board_score(prev_board, curr_board):\n",
        "    \"\"\"\n",
        "    Computes the score of the board given the current board and the previous one.\n",
        "    Also identifies the new letters added on the board.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prev_board : list\n",
        "        The board in the previous round.\n",
        "    curr_board : list\n",
        "        The board in the current round.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    score: int\n",
        "       The score of the board\n",
        "    new_letters: list\n",
        "      The list of the new letters and their possition\n",
        "    \"\"\"\n",
        "    no_letter_info = [] # the replacements for the letter found on the table (all '?' found)\n",
        "    new_letters = []\n",
        "    new_letters_temp = []\n",
        "    score = 0\n",
        "    for i in range(NO_SQUARES):\n",
        "        for j in range(NO_SQUARES):\n",
        "            # check if a letter is new on the board (is not present on the previous board)\n",
        "            if isinstance(curr_board[i][j], str) and curr_board[i][j] != prev_board[i][j] and curr_board[i][j] != '-':\n",
        "                new_letters.append([curr_board[i][j], i + 1, no_to_letter(j).upper()])\n",
        "                new_letters_temp.append([curr_board[i][j], i, j])\n",
        "            if isinstance(curr_board[i][j], str) and curr_board[i][j] == '-':\n",
        "                no_letter_info.append(['?', i, j])\n",
        "    new_letters, new_letters_temp = remove_false_new_letters(new_letters, new_letters_temp)\n",
        "    if len(new_letters) == 7:  # if all 7 letters have been used, add the bonus score\n",
        "        score += 50\n",
        "\n",
        "    # add '?' when the letter replacement is found, and make sure that the '?' found is not a recognition error\n",
        "    no_letter_info_final = []  # the real '?' added on the table if it exists\n",
        "    if len(no_letter_info) > 0:\n",
        "        if len(new_letters_temp) >= 2 and new_letters_temp[0][1] == new_letters_temp[1][1]:  # horizontal new word\n",
        "            for [l, i, j] in no_letter_info:\n",
        "                if i != new_letters_temp[0][1]:  # if the replacement is not on the same row with the newly added word, it must be an outsider\n",
        "                    continue\n",
        "                if not are_neigh_around(new_letters_temp, i, j):  # if the replacement has no neighbors, it must be an outsider\n",
        "                    continue\n",
        "                no_letter_info_final = ['?', i, j]\n",
        "            if no_letter_info_final != []:  # find the replacement that is near or between one of the newly added letters\n",
        "                for index_list in range(len(new_letters_temp) - 1):\n",
        "                    if new_letters_temp[index_list][2] < no_letter_info_final[2] and new_letters_temp[index_list+1][2] > no_letter_info_final[2]:\n",
        "                        new_letters_temp.insert(index_list, no_letter_info_final)\n",
        "                        new_letters.insert(index_list, [no_letter_info_final[0], no_letter_info_final[1] + 1, no_to_letter(no_letter_info_final[2]).upper()])\n",
        "    if len(no_letter_info) > 0:\n",
        "        if len(new_letters_temp) >= 2 and new_letters_temp[0][2] == new_letters_temp[1][2]:  # vertical new word\n",
        "            for [l, i, j] in no_letter_info:\n",
        "                if j != new_letters_temp[0][2]:    # if the replacement is not on the same col with the newly added word, it must be an outsider\n",
        "                    continue\n",
        "                if not are_neigh_around(new_letters_temp, i, j):  # if the replacement has no neighbors, it must be an outsider\n",
        "                    continue\n",
        "                no_letter_info_final = ['?', i, j]\n",
        "            if no_letter_info_final != []:  # find the replacement that is near or between one of the newly added letters\n",
        "                for index_list in range(len(new_letters_temp) - 1):\n",
        "                    if new_letters_temp[index_list][1] < no_letter_info_final[1] and new_letters_temp[index_list+1][1] > no_letter_info_final[1]:\n",
        "                        new_letters_temp.insert(index_list, no_letter_info_final)\n",
        "                        new_letters.insert(index_list, [no_letter_info_final[0], no_letter_info_final[1] + 1, no_to_letter(no_letter_info_final[2]).upper()])    \n",
        "\n",
        "    prev_vertical_words = []  # the words and their starting position that can be found on the vertical axes of the previous board\n",
        "    prev_horizontal_words = []  # the words and their starting position that can be found on the horizontal axes of the previous board\n",
        "    curr_vertical_words = []  # the words and their starting position that can be found on the vertical axes of the current board\n",
        "    curr_horizontal_words = []  # the words and their starting position that can be found on the horizontal axes of the current board\n",
        "\n",
        "    # compute the words and their starting position that can be found on the horizontal axes of the previous board\n",
        "    for i in range(NO_SQUARES):\n",
        "        j = 0\n",
        "        while j < NO_SQUARES:\n",
        "            if isinstance(prev_board[i][j], str):\n",
        "                j_start = j\n",
        "                j += 1\n",
        "                word = ''\n",
        "                word += prev_board[i][j_start]\n",
        "                while j < NO_SQUARES and (isinstance(prev_board[i][j], str) or prev_board[i][j] == '-'):\n",
        "                    word += prev_board[i][j]\n",
        "                    j += 1\n",
        "                if len(word) == 1:\n",
        "                    continue\n",
        "                else:\n",
        "                    prev_horizontal_words.append([word, i, j_start])\n",
        "            else:\n",
        "                  j += 1\n",
        "    # compute the words and their starting position that can be found on the horizontal axes of the current board  \n",
        "    for i in range(NO_SQUARES):\n",
        "        j = 0\n",
        "        while j < NO_SQUARES:\n",
        "            if isinstance(curr_board[i][j], str):\n",
        "                j_start = j\n",
        "                j += 1\n",
        "                word = ''\n",
        "                word += curr_board[i][j_start]\n",
        "                while j < NO_SQUARES and (isinstance(curr_board[i][j], str) or curr_board[i][j] == '-'):\n",
        "                    word += curr_board[i][j]\n",
        "                    j += 1\n",
        "                if len(word) == 1:\n",
        "                    continue\n",
        "                else:\n",
        "                    curr_horizontal_words.append([word, i, j_start])\n",
        "            else:\n",
        "                  j += 1\n",
        "    # compute the words and their starting position that can be found on the vertical axes of the previous board\n",
        "    for i in range(NO_SQUARES):\n",
        "        j = 0\n",
        "        while j < NO_SQUARES:\n",
        "            if isinstance(prev_board[j][i], str):\n",
        "                j_start = j\n",
        "                j += 1\n",
        "                word = ''\n",
        "                word += prev_board[j_start][i]\n",
        "                while j < NO_SQUARES and (isinstance(prev_board[j][i], str) or prev_board[j][i] == '-'):\n",
        "                    word += prev_board[j][i]\n",
        "                    j += 1\n",
        "                if len(word) == 1:\n",
        "                    continue\n",
        "                else:\n",
        "                    prev_vertical_words.append([word, j_start, i])\n",
        "            else:\n",
        "                  j += 1\n",
        "    # compute the words and their starting position that can be found on the vertical axes of the current board\n",
        "    for i in range(NO_SQUARES):\n",
        "        j = 0\n",
        "        while j < NO_SQUARES:\n",
        "            if isinstance(curr_board[j][i], str):\n",
        "                j_start = j\n",
        "                j += 1\n",
        "                word = ''\n",
        "                word += curr_board[j_start][i]\n",
        "                while j < NO_SQUARES and (isinstance(curr_board[j][i], str) or curr_board[j][i] == '-'):\n",
        "                    word += curr_board[j][i]\n",
        "                    j += 1\n",
        "                if len(word) == 1:\n",
        "                    continue\n",
        "                else:\n",
        "                    curr_vertical_words.append([word, j_start, i])\n",
        "            else:\n",
        "                  j += 1\n",
        "    # for each new horizontal word, compute the score\n",
        "    for [word, i, j] in curr_horizontal_words:\n",
        "        if [word, i, j] in prev_horizontal_words:\n",
        "            continue\n",
        "        s1 = 0\n",
        "        tr_word = 1\n",
        "        db_word = 1\n",
        "        for letter in word: # (1 -> triple word, 2 -> double word, 3 -> triple letter, 4 -> double letter, 0 otherwise)\n",
        "            if prev_board[i][j] == 1:\n",
        "                tr_word *= 3\n",
        "            if prev_board[i][j] == 2:\n",
        "                db_word *= 2\n",
        "            if prev_board[i][j] == 3:\n",
        "                s1 += letter_score[letter] * 3\n",
        "            elif prev_board[i][j] == 4:\n",
        "                s1 += letter_score[letter] * 2\n",
        "            else: \n",
        "                s1 += letter_score[letter]\n",
        "            j += 1\n",
        "        s1 *= db_word * tr_word\n",
        "        score += s1  # add the score of this word to the global score of this round\n",
        "\n",
        "    # for each new vertical word, compute the score\n",
        "    for [word, i, j] in curr_vertical_words:\n",
        "        if [word, i, j] in prev_vertical_words:\n",
        "            continue\n",
        "        s2 = 0\n",
        "        tr_word = 1\n",
        "        db_word = 1\n",
        "        for letter in word: # (1 -> triple word, 2 -> double word, 3 -> triple letter, 4 -> double letter, 0 otherwise)\n",
        "            if prev_board[i][j] == 1:\n",
        "                tr_word *= 3\n",
        "            if prev_board[i][j] == 2:\n",
        "                db_word *= 2\n",
        "            if prev_board[i][j] == 3:\n",
        "                s2 += letter_score[letter] * 3\n",
        "            elif prev_board[i][j] == 4:\n",
        "                s2 += letter_score[letter] * 2\n",
        "            else:\n",
        "                s2 += letter_score[letter]\n",
        "            i += 1\n",
        "        s2 *= db_word * tr_word\n",
        "        score += s2  # add the score of this word to the global score of this round\n",
        "\n",
        "    return score, new_letters"
      ],
      "metadata": {
        "id": "7cFPViRvJApk"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions for visualizing the output of the template matching and patch identification. These functions are not used in the final form of this notebook."
      ],
      "metadata": {
        "id": "YVxO53zHWhl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_lines(image, lines, color: tuple = (0, 0, 255)):\n",
        "    \"\"\"\n",
        "    Draws the lines on the image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : numpy array\n",
        "        The image we want to draw lines on\n",
        "    lines : list\n",
        "        The list of lines\n",
        "    color : tuple\n",
        "        The color of the lines\n",
        "    \"\"\"\n",
        "    drawing = image.copy()\n",
        "    if drawing.ndim == 2:\n",
        "        drawing = cv.cvtColor(drawing, cv.COLOR_GRAY2BGR)\n",
        "    for line in lines: \n",
        "        cv.line(drawing, line[0], line[1], color, 2, cv.LINE_AA)\n",
        "        \n",
        "    cv2_imshow(cv.resize(drawing, (0, 0), fx=0.15, fy=0.15))"
      ],
      "metadata": {
        "id": "EiPqDzoIRDxt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_patches(image, patches):\n",
        "    \"\"\"\n",
        "    Draws the patches on the image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : numpy array\n",
        "        The image we want to draw lines on\n",
        "    patches : list\n",
        "        The list of all patches\n",
        "    \"\"\"\n",
        "    patches_lines = []\n",
        "    for patch in patches:\n",
        "        [(x1, y1), (x2, y2)] = patch\n",
        "        r1 = [[(x1, y1), (x2, y1)]]\n",
        "        r2 = [[(x1, y2), (x2, y2)]]\n",
        "        c1 = [[(x1, y1), (x1, y2)]]\n",
        "        c2 = [[(x2, y1), (x2, y2)]]\n",
        "        patches_lines += r1 + r2 + c1 + c2\n",
        "    draw_lines(image, patches_lines)"
      ],
      "metadata": {
        "id": "gLcXrfiNVP2M"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_patch(image, patch):\n",
        "    \"\"\"\n",
        "    Draws a single patch on the image.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : numpy array\n",
        "        The image we want to draw lines on\n",
        "    patch : list\n",
        "        The list coordinates for the patch to be drawn\n",
        "    \"\"\"\n",
        "    [(x1, y1), (x2, y2)] = patch\n",
        "    r1 = [[(x1, y1), (x2, y1)]]\n",
        "    r2 = [[(x1, y2), (x2, y2)]]\n",
        "    c1 = [[(x1, y1), (x1, y2)]]\n",
        "    c2 = [[(x2, y1), (x2, y2)]]\n",
        "    draw_lines(image, r1+r2+c1+c2)"
      ],
      "metadata": {
        "id": "KdfLf3baUw8Z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manual identification of all the different patches from a chosen template. The function returns the coordinates of each patch."
      ],
      "metadata": {
        "id": "3ulOThwYWw-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_patches_from_template(padd_x=0, padd_y=0):\n",
        "    \"\"\"\n",
        "    Computes the patches for the template. The rows and columns coordinates are hardcoded.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    patches: list\n",
        "       A list of all the patches (top-left corner and bottom-right corner).\n",
        "    \"\"\"\n",
        "\n",
        "    l = [\n",
        "    [(0, 40), (2172, 40)],\n",
        "    [(0, 190), (2172, 190)],\n",
        "    [(0, 340), (2172, 340)],\n",
        "    [(0, 490), (2172, 490)],\n",
        "    [(0, 640), (2172, 640)],\n",
        "    [(0, 790), (2172, 790)],\n",
        "    [(0, 940), (2172, 940)],\n",
        "    [(0, 1090), (2172, 1090)],\n",
        "    [(0, 1240), (2172, 1240)],\n",
        "    [(0, 1390), (2172, 1390)],\n",
        "    [(0, 1540), (2172, 1540)],\n",
        "    [(0, 1690), (2172, 1690)],\n",
        "    [(0, 1840), (2172, 1840)],\n",
        "    [(0, 1990), (2172, 1990)],\n",
        "    [(0, 2140), (2172, 2140)],\n",
        "    [(0, 2290), (2172, 2290)]]\n",
        "\n",
        "    c = [\n",
        "    [(40, 0), (40, 2305)],\n",
        "    [(180, 0), (180, 2305)],\n",
        "    [(320, 0), (320, 2305)],\n",
        "    [(460, 0), (460, 2305)],\n",
        "    [(600, 0), (600, 2305)],\n",
        "    [(740, 0), (740, 2305)],\n",
        "    [(880, 0), (880, 2305)],\n",
        "    [(1020, 0), (1020, 2305)],\n",
        "    [(1160, 0), (1160, 2305)],\n",
        "    [(1300, 0), (1300, 2305)],\n",
        "    [(1440, 0), (1440, 2305)],\n",
        "    [(1580, 0), (1580, 2305)],\n",
        "    [(1720, 0), (1720, 2305)],\n",
        "    [(1860, 0), (1860, 2305)],\n",
        "    [(2000, 0), (2000, 2305)],\n",
        "    [(2140, 0), (2140, 2305)]]\n",
        "\n",
        "    patches = []\n",
        "    for i in range(0, len(l) - 1):\n",
        "        for j in range(0, len(c) - 1):\n",
        "            [(x1, y1), (_, _)] = l[i]\n",
        "            [(x2, y2), (_, _)] = c[j]\n",
        "            [(x3, y3), (_, _)] = l[i+1]\n",
        "            [(x4, y4), (_, _)] = c[j+1]\n",
        "            patches.append([(x2+padd_x, y1+padd_y), (x4+padd_x, y3+padd_y)])\n",
        "\n",
        "    return patches"
      ],
      "metadata": {
        "id": "ycZGw_19w-8s"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIFT\n",
        "\n",
        "The following functions are addapted from laboratory 4 and use SIFT features to first align the input image to the template and then to crop the input aligned image according to the template (so that we can use the patches identified before for each input image)."
      ],
      "metadata": {
        "id": "cID0B-ZwNFpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "f68f8c64"
      },
      "outputs": [],
      "source": [
        "def get_keypoints_and_features(image) -> tuple:\n",
        "    \"\"\"\n",
        "    Creates the SIFT object and find the keypoints from the grayscale image. Compute the features based on the grayscale image and the keypoints.\n",
        "    The code is addapted from Laboratory 4.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      image: numpy array\n",
        "          The image to be processed\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      keypoints: [cv.Keypoint] \n",
        "          The keypoints of the image\n",
        "      features: np.ndarray\n",
        "          The features for each keypoint.\n",
        "    \"\"\"\n",
        "    gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY) \n",
        "    \n",
        "    sift = cv.xfeatures2d.SIFT_create()\n",
        "    keypoints = sift.detect(gray_image, None)\n",
        "    keypoints, features = sift.compute(gray_image, keypoints) \n",
        "        \n",
        "    return keypoints, features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "edf33028"
      },
      "outputs": [],
      "source": [
        "def match_features(features_source, features_dest) -> [[cv.DMatch]]:\n",
        "    \"\"\"\n",
        "    Match features from the source image with the features from the destination image.\n",
        "    The code is addapted from Laboratory 4.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      features_source: np.ndarray\n",
        "          The features from the source image\n",
        "      features_dest: np.ndarray\n",
        "          The features from the destination image\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      matches: [[DMatch]]\n",
        "          The rusult of the matching. For each set of features from the source image,\n",
        "          it returns the first 'K' matchings from the destination images.\n",
        "    \"\"\"\n",
        "     \n",
        "    feature_matcher = cv.DescriptorMatcher_create(\"FlannBased\")\n",
        "    matches = feature_matcher.knnMatch(features_source, features_dest, k=2)   \n",
        "    return matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "05873cd2"
      },
      "outputs": [],
      "source": [
        "def generate_homography(all_matches:  [cv.DMatch], keypoints_source: [cv.KeyPoint], keypoints_dest : [cv.KeyPoint],\n",
        "                        ratio: float = 0.75, ransac_rep: int = 4.0):\n",
        "    \"\"\"\n",
        "    Find the matchings that pass the Lowe's ratio test (ratio parameter).\n",
        "    Get the coordinates of the keypoints from the source image.\n",
        "    Get the coordinates of the keypoints from the destination image.\n",
        "    Obtain the Homagraphy.\n",
        "    The code is addapted from Laboratory 4.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    all_matches : [DMatch]\n",
        "    keypoints_source : [cv.Point]\n",
        "    ratio : float\n",
        "      Lowe's ratio test (the ratio 1st neighbour distance / 2nd neighbour distance)\n",
        "    keypoints_source: nd.array [Nx2] (x, y coordinates)\n",
        "    keypoints_dest: nd.array [Nx2] (x, y coordinates)\n",
        "    ransac_rep: float\n",
        "      The threshold in the RANSAC algorithm.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    H\n",
        "      The homography matrix.\n",
        "    \"\"\"\n",
        "    if not all_matches:\n",
        "        return None\n",
        "    \n",
        "    matches = [] \n",
        "    for match in all_matches:  \n",
        "        if len(match) == 2 and (match[0].distance / match[1].distance) < ratio:\n",
        "            matches.append(match[0])\n",
        "     \n",
        "    points_source = np.float32([keypoints_source[m.queryIdx].pt for m in matches])\n",
        "    points_dest = np.float32([keypoints_dest[m.trainIdx].pt for m in matches])\n",
        "\n",
        "    if len(points_source) > 4:\n",
        "        H, status = cv.findHomography(points_source, points_dest, cv.RANSAC, ransac_rep)\n",
        "        return H\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_image(image_, procent=0.1):\n",
        "    \"\"\"\n",
        "    Padds an image.\n",
        "    The code is addapted from Laboratory 4.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      image_: np.ndarray\n",
        "          The image we want to padd\n",
        "      procent: float\n",
        "          The percentage to add as padding.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      big_image: np.ndarray\n",
        "          The padded image\n",
        "    \"\"\"\n",
        "    pad_h = int(image_.shape[0] * procent)\n",
        "    pad_w = int(image_.shape[1] * procent)\n",
        "    big_image = np.zeros((image_.shape[0] + 2 * pad_h, image_.shape[1] + pad_w, 3), np.uint8)\n",
        "    big_image[pad_h: pad_h + image_.shape[0], pad_w: pad_w + image_.shape[1]] = image_.copy()\n",
        "    return big_image"
      ],
      "metadata": {
        "id": "UWeCVH4POUz2"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "45d7e1bb"
      },
      "outputs": [],
      "source": [
        "def stitch_images_inside(image_source, image_dest):\n",
        "    \"\"\"\n",
        "    Get the keypoints and the features from the source image.\n",
        "    Get the keypoints and the features from the destination image.\n",
        "    Match the features.\n",
        "    Find the homography matrix.\n",
        "    Apply the homography matrix on the source image.\n",
        "    Copy the destination image in the resulting image from the previous point, but keep the resulting pixels in place!\n",
        "    The code is addapted from Laboratory 4.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      image_source: np.ndarray\n",
        "          The source image\n",
        "      image_source: np.ndarray\n",
        "          The destination image\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      result: np.ndarray\n",
        "          The stitched image\n",
        "    \"\"\"\n",
        " \n",
        "    image_dest = pad_image(image_dest)\n",
        "    keypoints_source, features_source = get_keypoints_and_features(image_source)\n",
        "    keypoints_dest, features_dest = get_keypoints_and_features(image_dest)\n",
        "          \n",
        "    all_matches = match_features(features_source, features_dest)\n",
        "        \n",
        "    H = generate_homography(all_matches, keypoints_source, keypoints_dest, 0.75, 4)\n",
        "\n",
        "    result = cv.warpPerspective(image_source, H, \n",
        "        (image_dest.shape[1], image_dest.shape[0]))\n",
        "    \n",
        "    mask = result[:, :] == [0, 0, 0]\n",
        "    result = result * (1 - mask) + image_dest * mask\n",
        "    return result.astype(np.uint8)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_image_as_template(image_source, image_dest):\n",
        "    \"\"\"\n",
        "    Get the keypoints and the features from the source image.\n",
        "    Get the keypoints and the features from the destination image.\n",
        "    Match the features.\n",
        "    Find the homography matrix.\n",
        "    Apply the homography matrix on the source image.\n",
        "    Crop the destination image according to the source image.\n",
        "    The code is addapted from Laboratory 4.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "      image_source: np.ndarray\n",
        "          The source image\n",
        "      image_source: np.ndarray\n",
        "          The destination image\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "      result: np.ndarray\n",
        "          The stitched image\n",
        "    \"\"\"\n",
        " \n",
        "    image_dest = pad_image(image_dest)\n",
        "    keypoints_source, features_source = get_keypoints_and_features(image_source)\n",
        "    keypoints_dest, features_dest = get_keypoints_and_features(image_dest)\n",
        "          \n",
        "    all_matches = match_features(features_source, features_dest)\n",
        "        \n",
        "    H = generate_homography(all_matches, keypoints_source, keypoints_dest, 0.75, 4)\n",
        "\n",
        "    result = cv.warpPerspective(image_source, H, \n",
        "        (image_dest.shape[1], image_dest.shape[0]))\n",
        "    \n",
        "    mask = result[:, :] == [0, 0, 0]\n",
        "\n",
        "    y, x, _ = np.nonzero(result) # crop the black edge\n",
        "    \n",
        "    return image_dest[np.min(y):np.max(y), np.min(x):np.max(x)]"
      ],
      "metadata": {
        "id": "-cFEHsDQiczO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN model"
      ],
      "metadata": {
        "id": "g1F8MkGM6QDj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate image patches to train a CNN model to classify them.\n",
        "\n",
        "For each train image, I have alligned it to the template, cropped it according to the template and then I have generated all the patches, using the function below. Each patch has been saved in a folder named 'new_images' and then used to train a model that classifies these patches."
      ],
      "metadata": {
        "id": "SQILIZJkXr8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cropped_images(image, patches, img_nr):\n",
        "    \"\"\"\n",
        "    Generate all images from all patches.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : numpy array\n",
        "        The image to be processed\n",
        "    patches : list\n",
        "        The list of all patches\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    \"\"\"\n",
        "    if not os.path.exists('/content/gdrive/MyDrive/new_images'):\n",
        "        os.makedirs('/content/gdrive/MyDrive/new_images')\n",
        "    indx = -1\n",
        "    for i in range(NO_SQUARES):\n",
        "        for j in range(NO_SQUARES):\n",
        "            indx += 1\n",
        "            [(x1, y1), (x2, y2)] = patches[indx]\n",
        "            cropped_image = image[y1:y2, x1:x2]\n",
        "            cv.imwrite('/content/gdrive/MyDrive/new_images/' + 'img_' + str(img_nr) + '_' + str(indx) + '_' + str(i) + '_' + str(j) + '.jpg', cropped_image)\n"
      ],
      "metadata": {
        "id": "kHlrA4tN5d9Z"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read all images from train and add them in images_train array\n",
        "base_folder = 'train'\n",
        "image_names = os.listdir(base_folder)\n",
        "image_names.sort()\n",
        "num_images = len(image_names) // 2\n",
        "images_train = []\n",
        "for img in image_names:\n",
        "    if img[-3:] == 'jpg':\n",
        "        image_now = cv.imread(os.path.join(base_folder, img))\n",
        "        images_train.append(image_now)"
      ],
      "metadata": {
        "id": "t-p2LJT-dXnc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform each image so that it maches a template, then gennerate all patches (uncomment to do so)\n",
        "# img_indx = -1\n",
        "# for img in images_train:\n",
        "#     img_indx += 1\n",
        "#     image_now = cv.imread('template.jpg')\n",
        "#     image_then = img\n",
        "#     stitched = stitch_images_inside(image_source=image_then, image_dest=image_now)\n",
        "#     image_then = cv.imread('template.jpg')\n",
        "#     image_now = stitched.astype(np.uint8)\n",
        "#     stitched2 = crop_image_as_template(image_source=image_then, image_dest=image_now)\n",
        "#     generate_cropped_images(stitched2, get_all_patches_from_template(335, 155), img_indx)"
      ],
      "metadata": {
        "id": "8-ICRHlH6Zyw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN model\n",
        "\n",
        "Based on the images that have been extracted previously (all the patches), I have manually classified a part of them and then trained a CNN to recognize each patch. The data used for training this CNN is located in Custom_Data.zip.\n",
        "\n",
        "There are 29 classes: one for each letter, one for the replacement piece, one for the empty patch and one for the paches that contain double/triple the score.\n",
        "\n",
        "The source of inspiration for the architecture of the CNN model was https://www.analyticsvidhya.com/blog/2020/02/learn-image-classification-cnn-convolutional-neural-networks-3-datasets/ which classifies the mnist dataset.\n",
        "\n",
        "Therefore, the architecture consists of 2 2D convolutional layers (one with 30 filters and one with 60 and a kernel of 3x3), followed by a max pooling layer of 2x2 and a dropout of 20%. Then another 2D convolutional layer of 120 filters followed by a max pooling layer and a dropout of 20%. Then the output is flattened and added into a dense layer (densely-connected NN) of 250 neurons, followed by a 30% dropout and another dense layer with softmax activation that does the final classification.\n",
        "\n",
        "The optimizer used for the model is Adam and the loss function is categorical_crossentropy as the labels are transformed into categorical data. The preprocessing for the input data is: the images are transformed into black and white images and then 3 gaussian blur filters are applied. The data is then reshaped and normalized and feeded into the model.\n",
        "\n",
        "The model was previously run and saved into MyBestModel2.zip so that we don't have to run it each time (the running time of the model is approximately 2h 30m. If it needs to be run again, the following code must be uncommented."
      ],
      "metadata": {
        "id": "eYVmXYFg6bkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip '/content/Custom_Data.zip'"
      ],
      "metadata": {
        "id": "dc2JFBcN6-4Z"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionaries used to decode and encode the labels of the data\n",
        "cat_to_label = {'A':1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'H':8,'I':9,'J':10,'K':11,'L':12,'M':13,'N':14,'O':15,'P':16,'Q':17,'R':18,'S':19,'T':20,'U':21,'V':22,'W':23,'X':24,'Y':25,'Z':26,'Nothing':27,'Double':28,'NoLetter':29}\n",
        "label_to_cat = {1:'A',2:'B',3:'C',4:'D',5:'E',6:'F',7:'G',8:'H',9:'I',10:'J',11:'K',12:'L',13:'M',14:'N',15:'O',16:'P',17:'Q',18:'R',19:'S',20:'T',21:'U',22:'V',23:'W',24:'X',25:'Y',26:'Z',27:'Nothing',28:'Double',29:'NoLetter'}"
      ],
      "metadata": {
        "id": "95ZTGZXE5xsY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_letter_data = {x : [] for x in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','Nothing','Double','NoLetter']}\n",
        "# base_folder = 'Custom_Data'\n",
        "# folder_names = os.listdir(base_folder)\n",
        "# for cat in folder_names:\n",
        "#     image_names = os.listdir(base_folder + '/' + cat)\n",
        "#     for img in image_names:\n",
        "#         if img[-3:] == 'jpg':\n",
        "#             my_img = cv.imread(os.path.join(base_folder, cat, img))\n",
        "#             my_img = cv.cvtColor(my_img, cv.COLOR_BGR2GRAY)\n",
        "#             my_img = cv.GaussianBlur(my_img, (3, 3), 0)\n",
        "#             my_img = cv.GaussianBlur(my_img, (3, 3), 0)\n",
        "#             my_img = cv.GaussianBlur(my_img, (3, 3), 0)\n",
        "#             train_letter_data[cat].append(my_img)"
      ],
      "metadata": {
        "id": "i4UOOAyE98l7"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_train_letter_data = []\n",
        "# all_train_letter_labels = []\n",
        "# for cat in train_letter_data:\n",
        "#     for img in train_letter_data[cat]:\n",
        "#         all_train_letter_data.append(img)\n",
        "#         all_train_letter_labels.append(cat_to_label[cat])"
      ],
      "metadata": {
        "id": "MZLGMCZp-A5F"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = np.array(all_train_letter_data)\n",
        "# Y_train = np.array(all_train_letter_labels)"
      ],
      "metadata": {
        "id": "iKQbq1VL-FSk"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_train = X_train.reshape(X_train.shape[0], 150, 140, 1)  # reshape data\n",
        "# X_train = X_train.astype('float32')\n",
        "# X_train /= 255  # normalize data\n",
        "# y_train = to_categorical(Y_train)  # transform the labeling into categorical\n",
        "\n",
        "# # construct the model\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(30, kernel_size=(3,3), padding='same', activation='relu', input_shape=(150, 140, 1)))\n",
        "# model.add(Conv2D(60, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# model.add(MaxPool2D(pool_size=(2,2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Conv2D(120, kernel_size=(3,3), padding='same', activation='relu'))\n",
        "# model.add(MaxPool2D(pool_size=(2,2)))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(250, activation='relu'))\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Dense(30, activation='softmax'))\n",
        "\n",
        "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # compile the model\n",
        "\n",
        "# model.fit(X_train, y_train, batch_size=128, epochs=20)  # fit the model"
      ],
      "metadata": {
        "id": "zlBU2QC5-FYl"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save('/content/gdrive/MyDrive/MyBestModel2')"
      ],
      "metadata": {
        "id": "qcCOBvU9-Mba"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the information from the board for each image and compute the results."
      ],
      "metadata": {
        "id": "Qn5FY9mwD2af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_letters_cnn(image, patches):\n",
        "    \"\"\"\n",
        "    Returns the matrix that encodes the image using a pretrained model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    image : numpy array\n",
        "        The image to be processed\n",
        "    patches : list\n",
        "        The list of all patches\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    template: list of lists\n",
        "       The matrix that encodes the image.\n",
        "    \"\"\"\n",
        "    template = get_board_template()\n",
        "    all_patches_to_decode = []\n",
        "    indx = -1\n",
        "    for i in range(NO_SQUARES):\n",
        "        for j in range(NO_SQUARES):\n",
        "            indx += 1\n",
        "            # get the patch and preprocess it\n",
        "            [(x1, y1), (x2, y2)] = patches[indx]\n",
        "            cropped_image = image[y1:y2, x1:x2]\n",
        "            cropped_image = cropped_image.astype(np.uint8)\n",
        "            cropped_image = cv.cvtColor(cropped_image, cv.COLOR_BGR2GRAY)\n",
        "            cropped_image = cv.GaussianBlur(cropped_image, (3, 3), 0)\n",
        "            cropped_image = cv.GaussianBlur(cropped_image, (3, 3), 0)\n",
        "            cropped_image = cv.GaussianBlur(cropped_image, (3, 3), 0)\n",
        "            all_patches_to_decode.append(cropped_image)\n",
        "\n",
        "    # use the model to predict the content of each patch\n",
        "    all_patches_to_decode = np.array(all_patches_to_decode)\n",
        "    all_patches_to_decode = all_patches_to_decode.reshape(all_patches_to_decode.shape[0], 150, 140, 1)\n",
        "    all_patches_to_decode = all_patches_to_decode.astype('float32')\n",
        "    all_patches_to_decode /= 255\n",
        "    y_pred = model_cnn.predict(all_patches_to_decode)\n",
        "\n",
        "    # transform the predictions back into our initial labels\n",
        "    predictions = []\n",
        "    for pred in y_pred:\n",
        "        pred_flag = False\n",
        "        for i, y in enumerate(pred):\n",
        "            if y > 0.5:\n",
        "                pred_flag = True\n",
        "                predictions.append(i)\n",
        "        if pred_flag == False:\n",
        "            predictions.append(27)\n",
        "\n",
        "    # add to the template the letters that were found\n",
        "    indx = -1\n",
        "    for i in range(NO_SQUARES):\n",
        "        for j in range(NO_SQUARES):\n",
        "            indx += 1\n",
        "            letter = label_to_cat[predictions[indx]]\n",
        "            if letter in ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']:\n",
        "                template[i][j] = letter\n",
        "            elif letter in ['NoLetter']:\n",
        "                template[i][j] = '-'\n",
        "    return template"
      ],
      "metadata": {
        "id": "G3yLIlqJ2H5t"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the output of each image in a folder saved on google drive (/content/gdrive/MyDrive/results)."
      ],
      "metadata": {
        "id": "XxWQUcb5cNqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_output(score, new_letters, num_game, num_round):\n",
        "    \"\"\"\n",
        "    Writes the output of each image into a text file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    score : float\n",
        "        The score computed from the image\n",
        "    new_letters : list\n",
        "        The list of the new letters and their positions\n",
        "    num_game : int\n",
        "        The number of the game (from 1 to 5)\n",
        "    num_round : int\n",
        "        The number of the num_round (from 1 to 20)    \n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    \"\"\"\n",
        "    if not os.path.exists('/content/gdrive/MyDrive/results'):\n",
        "        os.makedirs('/content/gdrive/MyDrive/results')\n",
        "    str_num_round = ''\n",
        "    if num_round < 10:\n",
        "        str_num_round = '0' + str(num_round)\n",
        "    else:\n",
        "        str_num_round = str(num_round)\n",
        "    f = open('/content/gdrive/MyDrive/results/' + str(num_game) + '_' + str_num_round + '.txt', 'w')\n",
        "    for [l, i, j] in new_letters:\n",
        "        f.write(str(i) + str(j) + ' ' + l + '\\n')\n",
        "    f.write(str(score))\n",
        "    f.close()"
      ],
      "metadata": {
        "id": "5YNcNWcHF8l2"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the solution to obtain the result\n",
        "\n",
        "Unzip the CNN model and load it, then for each image in the dataset, read it, allign it with the template and crop it according to the template, extract the information from each patch and compute the score and the new letters (and their positions) from the board. Then, write the output into a corresponding output file and continue with the next picture.\n",
        "\n",
        "Beacause we need the previous board to compute the score of the current board, we will keep track of it as well (at the beginning of each game, the previous board is just the template board)."
      ],
      "metadata": {
        "id": "sFCl_IECb_3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip MyBestModel2.zip"
      ],
      "metadata": {
        "id": "L3x8f82FNlS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_cnn = load_model('/content/gdrive/MyDrive/MyBestModel2')  # load the saved CNN model for patches recognition\n",
        "model_cnn = load_model('MyBestModel2')"
      ],
      "metadata": {
        "id": "6kmhOuxsCDT0"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read all images from train and add them in images_train array; for testing dataset just change train with test in BASE_FOLDER (defined at the beggining of this notebook)\n",
        "base_folder = BASE_FOLDER\n",
        "image_names = os.listdir(base_folder)\n",
        "image_names.sort()\n",
        "num_images = len(image_names) // 2\n",
        "images_train = []\n",
        "for img in image_names:\n",
        "    if img[-3:] == 'jpg':\n",
        "        image_now = cv.imread(os.path.join(base_folder, img))\n",
        "        images_train.append(image_now)"
      ],
      "metadata": {
        "id": "QdxGyuhwCeQB"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the output for each image\n",
        "num_img = -1\n",
        "prev_board = get_board_template()  # the board from the last round\n",
        "curr_board = get_board_template()  # the board from the current round\n",
        "for num_game in range(5):\n",
        "    prev_board = get_board_template()  # at the beginning of each game, the board must be empty\n",
        "    for num_round in range(20):\n",
        "        num_img += 1\n",
        "        img = images_train[num_img]  # get the current image\n",
        "        image_now = cv.imread('template.jpg')  # get the template image\n",
        "        image_then = img\n",
        "        stitched = stitch_images_inside(image_source=image_then, image_dest=image_now)  # allign the image with the template\n",
        "        image_then = cv.imread('template.jpg')\n",
        "        image_now = stitched.astype(np.uint8)\n",
        "        stitched2 = crop_image_as_template(image_source=image_then, image_dest=image_now)  # crop the image according to the template\n",
        "        curr_board = extract_letters_cnn(stitched2, get_all_patches_from_template(335, 155))  # extract the information from each patch\n",
        "        score, new_letters = board_score(prev_board, curr_board)  # compute the score and the new letters (and their positions) from the board\n",
        "        write_output(score, new_letters, num_game + 1, num_round + 1)  # write the output into a corresponding output file\n",
        "        prev_board = curr_board  # the current board will be the previous board in the next round"
      ],
      "metadata": {
        "id": "v1YxhGoY71YK"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the final score by comparing the files from the predictions with the files from the ground truth."
      ],
      "metadata": {
        "id": "SQCqKAxcdeLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_annotations(filename_predicted,filename_gt):\n",
        "    \"\"\"\n",
        "    Computes the final score of the assignment.\n",
        "    Code addapted from the assignment evaluation code.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename_predicted : str\n",
        "        The name of the filename that contains the predictions\n",
        "    filename_gt : str\n",
        "        The name of the filename that contains the ground truth \n",
        "\n",
        "    Returns\n",
        "    ----------\n",
        "    points_positions, points_letters, points_score : float\n",
        "        The 3 scores obtained with the photo\n",
        "    \"\"\"\n",
        "    p = open(filename_predicted,\"rt\")  \t\n",
        "    gt = open(filename_gt,\"rt\")  \t\n",
        "    all_lines_p = p.readlines()\n",
        "    all_lines_gt = gt.readlines()\n",
        "\n",
        "    number_lines_p = len(all_lines_p)\n",
        "    number_lines_gt = len(all_lines_gt)\n",
        "\n",
        "    match_positions = 1\n",
        "    match_letters = 1\n",
        "    match_score = 1\n",
        "\n",
        "    if number_lines_gt != number_lines_p:  # added by me, because the code fails if the number of identified letters is diffrent from gt\n",
        "        match_positions = 0\n",
        "        match_letters = 0\n",
        "    else:\n",
        "        for i in range(number_lines_gt-1):\n",
        "          current_pos_p, current_letter_p = all_lines_p[i].split()\n",
        "          current_pos_gt, current_letter_gt = all_lines_gt[i].split()\n",
        "          if(current_pos_p != current_pos_gt):\n",
        "            match_positions = 0\n",
        "          if(current_letter_p != current_letter_gt):\n",
        "            match_letters = 0\n",
        "    \n",
        "    points_positions = 0.025 * match_positions\n",
        "    points_letters = 0.015 * match_letters\t\n",
        "\n",
        "    #scores\n",
        "    last_line_p = all_lines_p[-1]\n",
        "    score_p = last_line_p.split()\n",
        "    last_line_gt= all_lines_gt[-1]\n",
        "    score_gt = last_line_gt.split()\n",
        "    if(score_p != score_gt):\n",
        "      match_score = 0\n",
        "\n",
        "    points_score = 0.01 * match_score\n",
        "\n",
        "    return points_positions, points_letters,points_score\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "predictions_path_root = \"/content/gdrive/MyDrive/results/\"\n",
        "gt_path_root = \"test/\"\n",
        "\n",
        "total_points = 0\n",
        "for game in range(1,6):\n",
        "\tfor turn in range(1,21):\n",
        "\t\t\n",
        "\t\tname_turn = str(turn)\n",
        "\t\tif(turn< 10):\n",
        "\t\t\tname_turn = '0'+str(turn)\n",
        "\n",
        "\t\tfilename_predicted = predictions_path_root + str(game) + '_' + name_turn + '.txt'\n",
        "\t\tfilename_gt = gt_path_root + str(game) + '_' + name_turn + '.txt'\n",
        "\t\tpoints_position, points_letters, points_score = compare_annotations(filename_predicted,filename_gt)\n",
        "\t\ttotal_points = total_points + points_position + points_letters + points_score\n",
        "\n",
        "print(total_points)"
      ],
      "metadata": {
        "id": "cUEWmtC_t4ME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8db8af-2b92-420a-8333-d073c4222490"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5649999999999964\n"
          ]
        }
      ]
    }
  ]
}